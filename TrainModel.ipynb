{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# some imports\n",
    "import os\n",
    "from functools import reduce\n",
    "import operator\n",
    "from skimage.draw import polygon\n",
    "from scipy import interpolate\n",
    "import numpy as np\n",
    "np.random.seed(seed=1)\n",
    "from glob import glob\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib notebook\n",
    "from natsort import natsorted\n",
    "import keras\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# import custom functions and viewing tools\n",
    "from VisTools import multi_slice_viewer0, mask_viewer0\n",
    "from KerasModel import BlockModel, dice_coef_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#~# some parameters to set for training #~#\n",
    "# path to save best model weights\n",
    "model_version = 4\n",
    "model_weights_path = os.path.join(os.getcwd(),\n",
    "                                  'BestModelWeights_v{:02d}.h5'.format(model_version))\n",
    "# set number of unique subjects to be used for testing\n",
    "test_num = 3\n",
    "# set number of unique subjects to to be used for validation\n",
    "val_num = 3\n",
    "# whether to use data augmentation or not\n",
    "augment = True\n",
    "# how many iterations of data to train on\n",
    "numEp = 100\n",
    "# augmentation factor\n",
    "augFact = 4\n",
    "\n",
    "# set data directory\n",
    "data_dir = os.path.join('/home','bashirmllab','output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44 total scans found\n",
      "16 unique subjects\n"
     ]
    }
   ],
   "source": [
    "# find unique subjects\n",
    "all_inputs = natsorted(glob(os.path.join(data_dir,\"input*.npy\")))\n",
    "all_targets = natsorted(glob(os.path.join(data_dir,\"target*.npy\")))\n",
    "stems = [f[:-12] for f in all_inputs]\n",
    "unq_stems = np.unique(stems)\n",
    "# get number of unique subjects\n",
    "numSubjs = len(unq_stems)\n",
    "print('{} total scans found'.format(len(all_inputs)))\n",
    "print('{} unique subjects'.format(numSubjs))\n",
    "# group repeated scans together\n",
    "groupings = [[i for i, e in enumerate(stems) if e == u] for u in unq_stems]\n",
    "grouped_inputs = [[all_inputs[g] for g in group] for group in groupings]\n",
    "grouped_targets = [[all_targets[g] for g in group] for group in groupings]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randaomly select test subject indices\n",
    "# numpy is seeded so this is repeatable\n",
    "tv_inds = np.random.choice(numSubjs,test_num+val_num,replace=False)\n",
    "test_inds = tv_inds[:test_num]\n",
    "val_inds = tv_inds[test_num:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The test file indices are:\n",
      "[9, 10, 11, 35, 36, 37, 19, 20, 21]\n"
     ]
    }
   ],
   "source": [
    "# split into test,train,validation\n",
    "# grab testing files and reduce to a single list\n",
    "input_files_test = reduce(operator.add,[grouped_inputs[i] for i in test_inds])\n",
    "target_files_test = reduce(operator.add,[grouped_targets[i] for i in test_inds])\n",
    "# grab validation files and reduce to a single list\n",
    "input_files_val = reduce(operator.add,[grouped_inputs[i] for i in val_inds])\n",
    "target_files_val = reduce(operator.add,[grouped_targets[i] for i in val_inds])\n",
    "# remove testing and validation files from lists\n",
    "# and take what's left as training files\n",
    "for i in tv_inds:\n",
    "    grouped_inputs.pop(i)\n",
    "    grouped_targets.pop(i)\n",
    "input_files_train = reduce(operator.add,grouped_inputs)\n",
    "target_files_train = reduce(operator.add,grouped_targets)\n",
    "# get list of file indices of test files for\n",
    "# future reference\n",
    "test_file_inds = reduce(operator.add,[groupings[i] for i in test_inds])\n",
    "print('The test file indices are:')\n",
    "print(test_file_inds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading input data...\n",
      "Input data loaded\n"
     ]
    }
   ],
   "source": [
    "# load input data\n",
    "print('Loading input data...')\n",
    "inputs_test = np.concatenate([np.load(f) for f in input_files_test])\n",
    "inputs_val = np.concatenate([np.load(f) for f in input_files_val])\n",
    "inputs_train = np.concatenate([np.load(f) for f in input_files_test])\n",
    "# add singleton dimension for grayscale channel\n",
    "testX = inputs_test[...,np.newaxis]\n",
    "valX = inputs_val[...,np.newaxis]\n",
    "trainX = inputs_train[...,np.newaxis]\n",
    "print('Input data loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading target data...\n",
      "Target data loaded\n"
     ]
    }
   ],
   "source": [
    "# load target data\n",
    "print('Loading target data...')\n",
    "targets_test = np.concatenate([np.load(f) for f in target_files_test])\n",
    "targets_val = np.concatenate([np.load(f) for f in target_files_val])\n",
    "targets_train = np.concatenate([np.load(f) for f in target_files_test])\n",
    "# add singleton dimension for grayscale channel\n",
    "testY = targets_test[...,np.newaxis]\n",
    "valY = targets_val[...,np.newaxis]\n",
    "trainY = targets_train[...,np.newaxis]\n",
    "print('Target data loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make model\n",
    "model = BlockModel(trainX.shape,filt_num=16,numBlocks=4)\n",
    "model.compile(optimizer=Adam(), loss=dice_coef_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup image data generator\n",
    "if augment:\n",
    "    datagen1 = ImageDataGenerator(\n",
    "        rotation_range=15,\n",
    "        shear_range=0.5,\n",
    "        width_shift_range=0.1,\n",
    "        height_shift_range=0.1,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        vertical_flip=True,\n",
    "        fill_mode='nearest')\n",
    "    datagen2 = ImageDataGenerator(\n",
    "        rotation_range=15,\n",
    "        shear_range=0.5,\n",
    "        width_shift_range=0.1,\n",
    "        height_shift_range=0.1,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        vertical_flip=True,\n",
    "        fill_mode='nearest')\n",
    "else:\n",
    "    datagen1 = ImageDataGenerator()\n",
    "    datagen2 = ImageDataGenerator()\n",
    "# Provide the same seed and keyword arguments to the fit and flow methods\n",
    "seed = 1\n",
    "datagen1.fit(trainX, seed=seed)\n",
    "datagen2.fit(trainY, seed=seed)\n",
    "batchsize = 16\n",
    "datagen = zip( datagen1.flow( trainX, None, batchsize, seed=seed), datagen2.flow( trainY, None, batchsize, seed=seed) )\n",
    "\n",
    "# calculate number of batches\n",
    "if augment:\n",
    "    steps = np.int(trainX.shape[0]/batchsize*augFact)\n",
    "else:\n",
    "    steps = np.int(trainX.shape[0]/batchsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make callback for checkpointing\n",
    "cb_check = ModelCheckpoint(model_weights_path,monitor='val_loss',\n",
    "                                   verbose=0,save_best_only=True,\n",
    "                                   save_weights_only=True,mode='auto',period=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "66/66 [==============================] - 45s 681ms/step - loss: 0.4782 - val_loss: 0.9964\n",
      "Epoch 2/100\n",
      "66/66 [==============================] - 29s 439ms/step - loss: 0.2818 - val_loss: 0.9083\n",
      "Epoch 3/100\n",
      "15/66 [=====>........................] - ETA: 20s - loss: 0.2682"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-08635d66d967>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcb_check\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m                     \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m                     validation_data=(valX,valY))\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/env_keras/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/env_keras/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1418\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1420\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/env_keras/lib/python3.6/site-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    215\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[1;32m    216\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m                                             class_weight=class_weight)\n\u001b[0m\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/env_keras/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1216\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1217\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/env_keras/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/env_keras/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/env_keras/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1397\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1398\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1399\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1400\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1401\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# train model\n",
    "history = model.fit_generator(datagen,\n",
    "                    steps_per_epoch=steps,\n",
    "                    epochs=numEp,\n",
    "                    callbacks=[cb_check],\n",
    "                    verbose=1,\n",
    "                    validation_data=(valX,valY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot loss curves\n",
    "epochs = np.arange(1,len(history.history['loss'])+1)\n",
    "plt.figure()\n",
    "plt.plot(epochs,history.history['loss'],'b-')\n",
    "plt.plot(epochs,history.history['val_loss'],'r-o')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('1-Dice')\n",
    "plt.ylim([0,1])\n",
    "plt.legend(['Training Loss','Validation Loss'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load best weights\n",
    "model.load_weights(model_weights_path)\n",
    "# evaluate on test data\n",
    "score = model.evaluate(testX,testY,verbose=0)\n",
    "print(\"Test Dice score is {:.03f}\".format(1-score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display some results\n",
    "output = model.predict(testX,batch_size=16)\n",
    "mask_viewer0(testX[...,0],testY[...,0],output[...,0],name='Test Results')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This set of cells can be run independently to get predicted volumes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from KerasModel import CalcVolumes\n",
    "from natsort import natsorted\n",
    "from glob import glob\n",
    "# set data directory\n",
    "data_dir = os.path.join('/home','johnsonj118','output')\n",
    "# set voxel dimensions (in cm)\n",
    "sx = .7\n",
    "sy = .1\n",
    "sz = .2\n",
    "vox_vol = sx*sy*sz\n",
    "# select which model to load\n",
    "model_version = 4\n",
    "# test file indices- copy and paste from\n",
    "# cell 5 output\n",
    "test_inds = [9, 10, 11, 35, 36, 37, 19, 20, 21]\n",
    "# get input/target files\n",
    "input_files= natsorted(glob(os.path.join(data_dir, \"input*.npy\")))\n",
    "target_files= natsorted(glob(os.path.join(data_dir, \"target*.npy\")))\n",
    "test_input_files = [input_files[i] for i in test_inds]\n",
    "test_target_files = [target_files[i] for i in test_inds]\n",
    "# create model\n",
    "from KerasModel import BlockModel\n",
    "model = BlockModel((1,512,512,1),filt_num=16,numBlocks=4)\n",
    "# load saved weights\n",
    "model_weights_path = os.path.join(os.getcwd(),'BestModelWeights_v{:02d}.h5'.format(model_version))\n",
    "model.load_weights(model_weights_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose subject to calculate volume of\n",
    "subj_ind = 3\n",
    "# run function to perform calculations\n",
    "calc,truth = CalcVolumes(input_files[subj_ind],target_files[subj_ind],vox_vol,model)\n",
    "# display result\n",
    "print(\"The calculated volume is {:.02f} cm^3\".format(calc))\n",
    "print(\"The actual volume is {:.02f} cm^3\".format(truth))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all volumes and plot correlation plot\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib notebook\n",
    "results = [CalcVolumes(inp,targ,vox_vol,model) for inp,targ in zip(test_input_files,test_target_files)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [r[1] for r in results]\n",
    "y = [r[0] for r in results]\n",
    "plt.figure()\n",
    "plt.plot(x,y,'ro')\n",
    "plt.plot(np.arange(0,np.max(x)),np.arange(0,np.max(x)),'k--')\n",
    "plt.xlim([0,np.max(x)])\n",
    "plt.ylim([0,np.max(y)])\n",
    "plt.ylabel('Calculated Volume (cc)')\n",
    "plt.xlabel('Actual Volume (cc)')\n",
    "plt.title(\"Correlation of Actual and Predicted Liver Volumes of Test Subjects\")\n",
    "plt.show()\n",
    "cc = np.corrcoef(x,y)[0,1]\n",
    "print('Correlation coefficient is {:.03f}'.format(cc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
