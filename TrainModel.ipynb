{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# some imports\n",
    "import os\n",
    "from functools import reduce\n",
    "import operator\n",
    "from skimage.draw import polygon\n",
    "from scipy import interpolate\n",
    "import numpy as np\n",
    "np.random.seed(seed=1)\n",
    "from glob import glob\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib notebook\n",
    "from natsort import natsorted\n",
    "import keras\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# import custom functions and viewing tools\n",
    "from VisTools import multi_slice_viewer0, mask_viewer0\n",
    "from KerasModel import BlockModel, dice_coef_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#~# some parameters to set for training #~#\n",
    "# path to save best model weights\n",
    "model_version = 3\n",
    "model_weights_path = os.path.join(os.getcwd(),\n",
    "                                  'BestModelWeights_v{:02d}.h5'.format(model_version))\n",
    "# set number of unique subjects to be used for testing\n",
    "test_num = 3\n",
    "# set number of unique subjects to to be used for validation\n",
    "val_num = 3\n",
    "# whether to use data augmentation or not\n",
    "augment = True\n",
    "# how many iterations of data to train on\n",
    "numEp = 100\n",
    "# augmentation factor\n",
    "augFact = 4\n",
    "\n",
    "# set data directory\n",
    "data_dir = os.path.join('/home','johnsonj118','output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44 total scans found\n",
      "16 unique subjects\n"
     ]
    }
   ],
   "source": [
    "# find unique subjects\n",
    "all_inputs = natsorted(glob(os.path.join(data_dir,\"input*.npy\")))\n",
    "all_targets = natsorted(glob(os.path.join(data_dir,\"target*.npy\")))\n",
    "stems = [f[:-12] for f in all_inputs]\n",
    "unq_stems = np.unique(stems)\n",
    "# get number of unique subjects\n",
    "numSubjs = len(unq_stems)\n",
    "print('{} total scans found'.format(len(all_inputs)))\n",
    "print('{} unique subjects'.format(numSubjs))\n",
    "# group repeated scans together\n",
    "groupings = [[i for i, e in enumerate(stems) if e == u] for u in unq_stems]\n",
    "grouped_inputs = [[all_inputs[g] for g in group] for group in groupings]\n",
    "grouped_targets = [[all_targets[g] for g in group] for group in groupings]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Randaomly select test subject indices\n",
    "# numpy is seeded so this is repeatable\n",
    "tv_inds = np.random.choice(numSubjs,test_num+val_num,replace=False)\n",
    "test_inds = tv_inds[:test_num]\n",
    "val_inds = tv_inds[test_num:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# split into test,train,validation\n",
    "# grab testing files and reduce to a single list\n",
    "input_files_test = reduce(operator.add,[grouped_inputs[i] for i in test_inds])\n",
    "target_files_test = reduce(operator.add,[grouped_targets[i] for i in test_inds])\n",
    "# grab validation files and reduce to a single list\n",
    "input_files_val = reduce(operator.add,[grouped_inputs[i] for i in val_inds])\n",
    "target_files_val = reduce(operator.add,[grouped_targets[i] for i in val_inds])\n",
    "# remove testing and validation files from lists\n",
    "# and take what's left as training files\n",
    "for i in tv_inds:\n",
    "    grouped_inputs.pop(i)\n",
    "    grouped_targets.pop(i)\n",
    "input_files_train = reduce(operator.add,grouped_inputs)\n",
    "target_files_train = reduce(operator.add,grouped_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading input data...\n",
      "Input data loaded\n"
     ]
    }
   ],
   "source": [
    "# load input data\n",
    "print('Loading input data...')\n",
    "inputs_test = np.concatenate([np.load(f) for f in input_files_test])\n",
    "inputs_val = np.concatenate([np.load(f) for f in input_files_val])\n",
    "inputs_train = np.concatenate([np.load(f) for f in input_files_test])\n",
    "# add singleton dimension for grayscale channel\n",
    "testX = inputs_test[...,np.newaxis]\n",
    "valX = inputs_val[...,np.newaxis]\n",
    "trainX = inputs_train[...,np.newaxis]\n",
    "print('Input data loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading target data...\n",
      "Target data loaded\n"
     ]
    }
   ],
   "source": [
    "# load target data\n",
    "print('Loading target data...')\n",
    "targets_test = np.concatenate([np.load(f) for f in target_files_test])\n",
    "targets_val = np.concatenate([np.load(f) for f in target_files_val])\n",
    "targets_train = np.concatenate([np.load(f) for f in target_files_test])\n",
    "# add singleton dimension for grayscale channel\n",
    "testY = targets_test[...,np.newaxis]\n",
    "valY = targets_val[...,np.newaxis]\n",
    "trainY = targets_train[...,np.newaxis]\n",
    "print('Target data loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# make model\n",
    "model = BlockModel(trainX.shape,filt_num=16,numBlocks=4)\n",
    "model.compile(optimizer=Adam(), loss=dice_coef_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# setup image data generator\n",
    "if augment:\n",
    "    datagen1 = ImageDataGenerator(\n",
    "        rotation_range=15,\n",
    "        shear_range=0.5,\n",
    "        width_shift_range=0.1,\n",
    "        height_shift_range=0.1,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        vertical_flip=True,\n",
    "        fill_mode='nearest')\n",
    "    datagen2 = ImageDataGenerator(\n",
    "        rotation_range=15,\n",
    "        shear_range=0.5,\n",
    "        width_shift_range=0.1,\n",
    "        height_shift_range=0.1,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        vertical_flip=True,\n",
    "        fill_mode='nearest')\n",
    "else:\n",
    "    datagen1 = ImageDataGenerator()\n",
    "    datagen2 = ImageDataGenerator()\n",
    "# Provide the same seed and keyword arguments to the fit and flow methods\n",
    "seed = 1\n",
    "datagen1.fit(trainX, seed=seed)\n",
    "datagen2.fit(trainY, seed=seed)\n",
    "batchsize = 16\n",
    "datagen = zip( datagen1.flow( trainX, None, batchsize, seed=seed), datagen2.flow( trainY, None, batchsize, seed=seed) )\n",
    "\n",
    "# calculate number of batches\n",
    "if augment:\n",
    "    steps = np.int(trainX.shape[0]/batchsize*augFact)\n",
    "else:\n",
    "    steps = np.int(trainX.shape[0]/batchsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# make callback for checkpointing\n",
    "cb_check = ModelCheckpoint(model_weights_path,monitor='val_loss',\n",
    "                                   verbose=0,save_best_only=True,\n",
    "                                   save_weights_only=True,mode='auto',period=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model\n",
    "history = model.fit_generator(datagen,\n",
    "                    steps_per_epoch=steps,\n",
    "                    epochs=numEp,\n",
    "                    callbacks=[cb_check],\n",
    "                    verbose=1,\n",
    "                    validation_data=(valX,valY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot loss curves\n",
    "plt.plot(history['epochs'],history['loss'],'b-')\n",
    "plt.plot(history['epochs'],history['val_loss'],'ro')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('1-Dice')\n",
    "plt.ylim([0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Dice score is 0.948\n"
     ]
    }
   ],
   "source": [
    "# load best weights\n",
    "model.load_weights(model_weights_path)\n",
    "# evaluate on test data\n",
    "score = model.evaluate(testX,testY,verbose=0)\n",
    "print(\"Test Dice score is {:.03f}\".format(1-score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display some results\n",
    "output = model.predict(testX,batch_size=16)\n",
    "mask_viewer0(testX[...,0],testY[...,0],output[...,0],name='Test Results')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This set of cells can be run independently to get predicted volumes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from KerasModel import CalcVolumes\n",
    "from natsort import natsorted\n",
    "from glob import glob\n",
    "# set data directory\n",
    "data_dir = os.path.join('/home','johnsonj118','output')\n",
    "# set voxel dimensions (in cm)\n",
    "sx = .7\n",
    "sy = .1\n",
    "sz = .2\n",
    "vox_vol = sx*sy*sz\n",
    "# select which model to load\n",
    "model_version = 3\n",
    "# get input/target files\n",
    "input_files= natsorted(glob(os.path.join(data_dir, \"input*.npy\")))\n",
    "target_files= natsorted(glob(os.path.join(data_dir, \"input*.npy\")))\n",
    "# create model\n",
    "from KerasModel import BlockModel\n",
    "model = BlockModel((1,512,512,1),filt_num=16,numBlocks=4)\n",
    "# load saved weights\n",
    "model_weights_path = os.path.join(os.getcwd(),'BestModelWeights_v{:02d}.h5'.format(model_version))\n",
    "model.load_weights(model_weights_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose subject to calculate volume of\n",
    "subj_ind = 4\n",
    "# run function to perform calculations\n",
    "calc,truth = CalcVolumes(input_files[subj_ind],target_files[subj_ind],vox_vol,model)\n",
    "# display result\n",
    "print(\"The calculated volume is {:.02f} cm^3\".format(calc))\n",
    "print(\"The actual volume is {:.02f} cm^3\".format(truth))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get all volumes and plot correlation plot\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib notebook\n",
    "results = [CalcVolumes(inp,targ,vox_vol,model) for inp,targ in zip(input_files,target_files)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [r[1] for r in results]\n",
    "y = [r[0] for r in results]\n",
    "plt.plot(x,y,'ro')\n",
    "plt.xlim([0,np.max(x)])\n",
    "plt.ylim([0,np.max(y)])\n",
    "plt.ylabel('Calculated Volume (cc)')\n",
    "plt.xlabel('Actual Volume (cc)')\n",
    "plt.show()\n",
    "cc = np.corrcoef(x,y)[0,1]\n",
    "print('Correlation coefficient is {:.03f}'.format(cc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
