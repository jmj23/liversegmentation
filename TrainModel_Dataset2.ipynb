{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# some imports\n",
    "import os\n",
    "from functools import reduce\n",
    "import operator\n",
    "from skimage.draw import polygon\n",
    "from scipy import interpolate\n",
    "import numpy as np\n",
    "np.random.seed(seed=1)\n",
    "from glob import glob\n",
    "%matplotlib notebook\n",
    "from matplotlib import pyplot as plt\n",
    "from natsort import natsorted\n",
    "import keras\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# import custom functions and viewing tools\n",
    "from VisTools import multi_slice_viewer0, mask_viewer0\n",
    "from KerasModel import BlockModel, dice_coef_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#~# some parameters to set for training #~#\n",
    "# path to save best model weights\n",
    "model_version = 1\n",
    "model_weights_path = os.path.join(os.getcwd(),\n",
    "                                  'BestModelWeights_t1nfs_v{:02d}.h5'.format(model_version))\n",
    "# set number of unique subjects to be used for testing\n",
    "test_num = 8\n",
    "# set number of unique subjects to to be used for validation\n",
    "val_num = 8\n",
    "# whether to use data augmentation or not\n",
    "augment = True\n",
    "# how many iterations of data to train on\n",
    "numEp = 50\n",
    "# augmentation factor\n",
    "augFact = 2\n",
    "\n",
    "# set data directories\n",
    "dataset_dir = os.path.join('/home','bashirmllab','dataset2')\n",
    "subdirs = ['opposed','SSFSE','t1nfs']\n",
    "data_dirs = [os.path.join(dataset_dir,d+'_output') for d in subdirs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For opposed directory:\n",
      "47 total scans found\n",
      "38 unique subjects\n",
      "For SSFSE directory:\n",
      "132 total scans found\n",
      "54 unique subjects\n",
      "For t1nfs directory:\n",
      "80 total scans found\n",
      "42 unique subjects\n",
      "----------------------------------\n",
      "259 total scans found\n",
      "134 total unique subjects found\n"
     ]
    }
   ],
   "source": [
    "# find unique subjects\n",
    "cur_dataset_ind = 0\n",
    "all_groupings = []\n",
    "all_grouped_inputs = []\n",
    "all_grouped_targets = []\n",
    "all_numScans = 0\n",
    "all_numSubjs = 0\n",
    "for cur_data_dir,cur_subdir in zip(data_dirs,subdirs):\n",
    "    all_inputs = natsorted(glob(os.path.join(cur_data_dir,\"input*.npy\")))\n",
    "    all_targets = natsorted(glob(os.path.join(cur_data_dir,\"target*.npy\")))\n",
    "    stem_length = len(cur_subdir)+7\n",
    "    stems = [f[:-stem_length] for f in all_inputs]\n",
    "    unq_stems = np.unique(stems)\n",
    "    # get number of unique subjects\n",
    "    numSubjs = len(unq_stems)\n",
    "    print('For {} directory:'.format(cur_subdir))\n",
    "    print('{} total scans found'.format(len(all_inputs)))\n",
    "    print('{} unique subjects'.format(numSubjs))\n",
    "    # group repeated scans together\n",
    "    groupings = [[i for i, e in enumerate(stems) if e == u] for u in unq_stems]\n",
    "    grouped_inputs = [[all_inputs[g] for g in group] for group in groupings]\n",
    "    grouped_targets = [[all_targets[g] for g in group] for group in groupings]\n",
    "    # add to cumulative lists\n",
    "    all_groupings += groupings\n",
    "    all_grouped_inputs += grouped_inputs\n",
    "    all_grouped_targets += grouped_targets\n",
    "    all_numScans += len(all_inputs)\n",
    "    all_numSubjs += numSubjs\n",
    "print('----------------------------------')\n",
    "print('{} total scans found'.format(all_numScans))\n",
    "print('{} total unique subjects found'.format(all_numSubjs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randaomly select test subject indices\n",
    "# numpy is seeded so this is repeatable\n",
    "tv_inds = np.random.choice(all_numSubjs,test_num+val_num,replace=False)\n",
    "test_inds = tv_inds[:test_num]\n",
    "val_inds = tv_inds[test_num:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: if you need to rerun this following cell, run cells above this one first.\n",
    "\n",
    "The code in this cell changes the input/target file lists so those lists need to be remade first before running this cell again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into test,train,validation\n",
    "# grab testing files and reduce to a single list\n",
    "input_files_test = reduce(operator.add,[all_grouped_inputs[i] for i in test_inds])\n",
    "target_files_test = reduce(operator.add,[all_grouped_targets[i] for i in test_inds])\n",
    "# grab validation files and reduce to a single list\n",
    "input_files_val = reduce(operator.add,[all_grouped_inputs[i] for i in val_inds])\n",
    "target_files_val = reduce(operator.add,[all_grouped_targets[i] for i in val_inds])\n",
    "# remove testing and validation files from lists\n",
    "# and take what's left as training files\n",
    "for i in tv_inds:\n",
    "    all_grouped_inputs.pop(i)\n",
    "    all_grouped_targets.pop(i)\n",
    "input_files_train = reduce(operator.add,all_grouped_inputs)\n",
    "target_files_train = reduce(operator.add,all_grouped_targets)\n",
    "# get list of file indices of test files for\n",
    "# future reference\n",
    "# test_file_inds = reduce(operator.add,[groupings[i] for i in test_inds])\n",
    "# print('The test file indices are:')\n",
    "# print(test_file_inds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading input data...\n",
      "Input data loaded\n"
     ]
    }
   ],
   "source": [
    "# load input data\n",
    "print('Loading input data...')\n",
    "inputs_test = np.concatenate([np.load(f) for f in input_files_test])\n",
    "inputs_val = np.concatenate([np.load(f) for f in input_files_val])\n",
    "inputs_train = np.concatenate([np.load(f) for f in input_files_train])\n",
    "# add singleton dimension for grayscale channel\n",
    "testX = inputs_test[...,np.newaxis]\n",
    "valX = inputs_val[...,np.newaxis]\n",
    "trainX = inputs_train[...,np.newaxis]\n",
    "print('Input data loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading target data...\n",
      "Target data loaded\n"
     ]
    }
   ],
   "source": [
    "# load target data\n",
    "print('Loading target data...')\n",
    "targets_test = np.concatenate([np.load(f) for f in target_files_test])\n",
    "targets_val = np.concatenate([np.load(f) for f in target_files_val])\n",
    "targets_train = np.concatenate([np.load(f) for f in target_files_train])\n",
    "# add singleton dimension for grayscale channel\n",
    "testY = targets_test[...,np.newaxis]\n",
    "valY = targets_val[...,np.newaxis]\n",
    "trainY = targets_train[...,np.newaxis]\n",
    "print('Target data loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make model\n",
    "model = BlockModel(trainX.shape,filt_num=16,numBlocks=4)\n",
    "model.compile(optimizer=Adam(), loss=dice_coef_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup image data generator\n",
    "if augment:\n",
    "    datagen1 = ImageDataGenerator(\n",
    "        rotation_range=10,\n",
    "        shear_range=0.5,\n",
    "        width_shift_range=0.1,\n",
    "        height_shift_range=0.1,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        vertical_flip=True,\n",
    "        fill_mode='nearest')\n",
    "    datagen2 = ImageDataGenerator(\n",
    "        rotation_range=10,\n",
    "        shear_range=0.5,\n",
    "        width_shift_range=0.1,\n",
    "        height_shift_range=0.1,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        vertical_flip=True,\n",
    "        fill_mode='nearest')\n",
    "else:\n",
    "    datagen1 = ImageDataGenerator()\n",
    "    datagen2 = ImageDataGenerator()\n",
    "# Provide the same seed and keyword arguments to the fit and flow methods\n",
    "seed = 1\n",
    "datagen1.fit(trainX, seed=seed)\n",
    "datagen2.fit(trainY, seed=seed)\n",
    "batchsize = 16\n",
    "datagen = zip( datagen1.flow( trainX, None, batchsize, seed=seed), datagen2.flow( trainY, None, batchsize, seed=seed) )\n",
    "\n",
    "# calculate number of batches\n",
    "if augment:\n",
    "    steps = np.int(trainX.shape[0]/batchsize*augFact)\n",
    "else:\n",
    "    steps = np.int(trainX.shape[0]/batchsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make callback for checkpointing\n",
    "cb_check = ModelCheckpoint(model_weights_path,monitor='val_loss',\n",
    "                                   verbose=0,save_best_only=True,\n",
    "                                   save_weights_only=True,mode='auto',period=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1136/1136 [==============================] - 506s 445ms/step - loss: 0.4255 - val_loss: 0.7746\n",
      "Epoch 2/50\n",
      "1136/1136 [==============================] - 497s 437ms/step - loss: 0.2588 - val_loss: 0.7560\n",
      "Epoch 3/50\n",
      "1136/1136 [==============================] - 506s 446ms/step - loss: 0.2204 - val_loss: 0.3857\n",
      "Epoch 4/50\n",
      " 990/1136 [=========================>....] - ETA: 1:05 - loss: 0.1990"
     ]
    }
   ],
   "source": [
    "# train model\n",
    "history = model.fit_generator(datagen,\n",
    "                    steps_per_epoch=steps,\n",
    "                    epochs=numEp,\n",
    "                    callbacks=[cb_check],\n",
    "                    verbose=1,\n",
    "                    validation_data=(valX,valY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot loss curves\n",
    "epochs = np.arange(1,len(history.history['loss'])+1)\n",
    "plt.figure()\n",
    "plt.plot(epochs,history.history['loss'],'b-')\n",
    "plt.plot(epochs,history.history['val_loss'],'r-o')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('1-Dice')\n",
    "plt.ylim([0,1])\n",
    "plt.legend(['Training Loss','Validation Loss'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load best weights\n",
    "model.load_weights(model_weights_path)\n",
    "# evaluate on test data\n",
    "score = model.evaluate(testX,testY,verbose=0)\n",
    "print(\"Test Dice score is {:.03f}\".format(1-score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display some results\n",
    "output = model.predict(testX,batch_size=16)\n",
    "mask_viewer0(testX[...,0],testY[...,0],output[...,0],name='Test Results')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This set of cells can be run independently to get predicted volumes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from KerasModel import CalcVolumes\n",
    "from natsort import natsorted\n",
    "from glob import glob\n",
    "# set data directory\n",
    "data_dir = os.path.join('/home','johnsonj118','output')\n",
    "# set voxel dimensions (in cm)\n",
    "sx = .7\n",
    "sy = .1\n",
    "sz = .2\n",
    "vox_vol = sx*sy*sz\n",
    "# select which model to load\n",
    "model_version = 4\n",
    "# test file indices- copy and paste from\n",
    "# cell 5 output\n",
    "test_inds = [9, 10, 11, 35, 36, 37, 19, 20, 21]\n",
    "# get input/target files\n",
    "input_files= natsorted(glob(os.path.join(data_dir, \"input*.npy\")))\n",
    "target_files= natsorted(glob(os.path.join(data_dir, \"target*.npy\")))\n",
    "test_input_files = [input_files[i] for i in test_inds]\n",
    "test_target_files = [target_files[i] for i in test_inds]\n",
    "# create model\n",
    "from KerasModel import BlockModel\n",
    "model = BlockModel((1,512,512,1),filt_num=16,numBlocks=4)\n",
    "# load saved weights\n",
    "model_weights_path = os.path.join(os.getcwd(),'BestModelWeights_v{:02d}.h5'.format(model_version))\n",
    "model.load_weights(model_weights_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose subject to calculate volume of\n",
    "subj_ind = 3\n",
    "# run function to perform calculations\n",
    "calc,truth = CalcVolumes(input_files[subj_ind],target_files[subj_ind],vox_vol,model)\n",
    "# display result\n",
    "print(\"The calculated volume is {:.02f} cm^3\".format(calc))\n",
    "print(\"The actual volume is {:.02f} cm^3\".format(truth))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all volumes and plot correlation plot\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib notebook\n",
    "results = [CalcVolumes(inp,targ,vox_vol,model) for inp,targ in zip(test_input_files,test_target_files)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [r[1] for r in results]\n",
    "y = [r[0] for r in results]\n",
    "plt.figure()\n",
    "plt.plot(x,y,'ro')\n",
    "plt.plot(np.arange(0,np.max(x)),np.arange(0,np.max(x)),'k--')\n",
    "plt.xlim([0,np.max(x)])\n",
    "plt.ylim([0,np.max(y)])\n",
    "plt.ylabel('Calculated Volume (cc)')\n",
    "plt.xlabel('Actual Volume (cc)')\n",
    "plt.title(\"Correlation of Actual and Predicted Liver Volumes of Test Subjects\")\n",
    "plt.show()\n",
    "cc = np.corrcoef(x,y)[0,1]\n",
    "print('Correlation coefficient is {:.03f}'.format(cc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
